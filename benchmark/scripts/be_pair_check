#!/usr/bin/env python
import argparse
from benchmark.Results import PairCheck
from benchmark.Utils import EnvDefault

"""Check best results of two models giving scores and win-tie-loose results
"""


def parse_arguments():
    ap = argparse.ArgumentParser()
    ap.add_argument(
        "-s",
        "--score",
        action=EnvDefault,
        envvar="score",
        type=str,
        required=True,
        help="score name {accuracy, f1_macro, ...}",
    )
    ap.add_argument(
        "-m1",
        "--model1",
        type=str,
        required=True,
        help="model 1 name",
    )
    ap.add_argument(
        "-m2",
        "--model2",
        type=str,
        required=True,
        help="model 2 name",
    )
    ap.add_argument(
        "-w",
        "--win",
        type=bool,
        default=False,
        required=False,
        help="show win results",
    )
    ap.add_argument(
        "-l",
        "--lose",
        type=bool,
        default=False,
        required=False,
        help="show lose results",
    )
    args = ap.parse_args()
    return (
        args.score,
        args.model1,
        args.model2,
        args.win,
        args.lose,
    )


if __name__ == "__main__":
    (
        score,
        model1,
        model2,
        win_results,
        lose_results,
    ) = parse_arguments()
    pair_check = PairCheck(score, model1, model2, win_results, lose_results)
    pair_check.compute()
    pair_check.report()
