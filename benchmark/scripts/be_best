#!/usr/bin/env python
import argparse
import json
from benchmark.Results import Summary
from benchmark.Utils import EnvDefault, ALL_METRICS


def parse_arguments():
    ap = argparse.ArgumentParser()
    ap.add_argument(
        "-s",
        "--score",
        type=str,
        action=EnvDefault,
        envvar="score",
        required=True,
        choices=ALL_METRICS,
        help="score name {accuracy, f1-macro, f1-weighted, roc-auc-ovr}",
    )
    args = ap.parse_args()
    return (args.score,)


(score,) = parse_arguments()

metrics = ALL_METRICS if score == "all" else [score]

summary = Summary()
summary.acquire()

nl = 50
num = 100
for metric in metrics:
    title = f"BEST RESULTS of {metric} for datasets"
    best = summary.best_results_datasets(score=metric)
    for key, item in best.items():
        print(f"{key:30s} {item[2]:{nl}s}")
        print("-" * num)
        print(f"{item[0]:30.7f} {json.dumps(item[1]):{nl}s}")
        print("-" * num)
        print(f"{item[3]:{nl+num}s}")
        print("*" * num)
