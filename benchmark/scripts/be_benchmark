#!/usr/bin/env python
from benchmark.Results import Benchmark
from benchmark.Utils import ALL_METRICS, Files, EnvDefault
import argparse


def parse_arguments():
    ap = argparse.ArgumentParser()
    ap.add_argument(
        "-s",
        "--score",
        action=EnvDefault,
        envvar="score",
        type=str,
        required=True,
        choices=ALL_METRICS,
        help="score name {accuracy, f1_macro, ...}",
    )
    ap.add_argument(
        "-x",
        "--excel",
        type=bool,
        required=False,
        help="Generate Excel File",
    )
    ap.add_argument(
        "-t",
        "--tex-output",
        type=bool,
        required=False,
        default=False,
    )
    args = ap.parse_args()
    return (args.score, args.excel, args.tex_output)


(score, excel, tex_output) = parse_arguments()
benchmark = Benchmark(score=score, visualize=True)
benchmark.compile_results()
benchmark.save_results()
benchmark.report(tex_output)
benchmark.exreport()
if excel:
    benchmark.excel()
    Files.open(benchmark.get_excel_file_name())
if tex_output:
    print(f"File {benchmark.get_tex_file()} generated")
